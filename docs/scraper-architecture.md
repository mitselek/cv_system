# Job Scraper Architecture Proposal

**Date:** December 2, 2025  
**Status:** Design Proposal  
**Problem:** Avoiding "the mess" when maintaining multiple scrapers with different approaches

## Current State Analysis

### Existing Structure

```
job-monitoring/src/job_monitor/
‚îú‚îÄ‚îÄ scraper.py          # Single class with all scrapers
‚îú‚îÄ‚îÄ cli.py              # Hardcoded source routing
‚îú‚îÄ‚îÄ config.py           # Generic config loading
‚îî‚îÄ‚îÄ schemas.py          # Shared data models
```

### Current Problems

1. **Single Monolithic Class**
   - `JobScraper` contains methods for all portals
   - `search_duunitori()`, `search_linkedin()`, etc. in one file
   - Each portal needs different dependencies (BeautifulSoup vs JSON API)

2. **Hardcoded Routing**
   - `cli.py` has string matching: `if "duunitori" in name`
   - Adding new portal = editing multiple files
   - No clear extension point

3. **Mixed Concerns**
   - Cookie handling (Duunitori needs it)
   - REST API calls (CV.ee doesn't need cookies)
   - HTML parsing vs JSON parsing
   - Rate limiting strategies differ per portal

4. **Future Pain Points**
   - **Duunitori:** HTML scraping with cookies, needs BeautifulSoup
   - **CV.ee:** REST API, no auth, JSON responses
   - **LinkedIn:** Complex auth, might need Selenium
   - **cvonline.lt/cv.lv:** Same API as CV.ee but different URLs
   - Each evolves independently on their own schedule

## Design Principles

### 1. Plugin Architecture
Each scraper is a **self-contained module** that can be:
- Developed independently
- Tested in isolation
- Enabled/disabled without code changes
- Replaced without affecting others

### 2. Clear Abstraction
Define **what** each scraper must do (interface), not **how** they do it:
- Search for jobs with query parameters
- Return standardized `JobPosting` objects
- Handle their own rate limiting
- Manage their own authentication

### 3. Configuration-Driven
Scraper selection based on **configuration**, not code:
- Add new scraper = drop in new file + config entry
- No editing existing code
- Runtime discovery of available scrapers

### 4. Shared Infrastructure
Common utilities available to all scrapers:
- HTTP session management
- Rate limiting decorators
- Caching
- Error handling patterns

## Proposed Architecture

### Directory Structure

```
job-monitoring/src/job_monitor/
‚îú‚îÄ‚îÄ scrapers/                    # NEW: Scraper plugins
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Scraper registry
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ duunitori.py            # Duunitori scraper
‚îÇ   ‚îú‚îÄ‚îÄ cvee.py                 # CV.ee scraper
‚îÇ   ‚îú‚îÄ‚îÄ cvonline.py             # cv.lv + cvonline.lt (shared API)
‚îÇ   ‚îî‚îÄ‚îÄ linkedin.py             # LinkedIn scraper
‚îú‚îÄ‚îÄ scraper.py                   # DEPRECATED: Keep for backward compat
‚îú‚îÄ‚îÄ cli.py                       # Update to use registry
‚îú‚îÄ‚îÄ config.py                    # No changes needed
‚îî‚îÄ‚îÄ schemas.py                   # Add ScraperConfig model
```

### Base Scraper Interface

```python
# scrapers/base.py
from abc import ABC, abstractmethod
from typing import List, Optional, Any
from pathlib import Path
from job_monitor.schemas import JobPosting

class BaseScraper(ABC):
    """Abstract base class for all job scrapers."""
    
    # Class-level metadata
    SCRAPER_ID: str          # e.g., "cvee", "duunitori"
    DISPLAY_NAME: str        # e.g., "CV.ee", "Duunitori"
    REQUIRES_COOKIES: bool = False
    REQUIRES_AUTH: bool = False
    BASE_URLS: List[str]     # e.g., ["https://cv.ee"]
    
    def __init__(self, config: dict[str, Any], cookies_file: Optional[Path] = None):
        """
        Initialize scraper with configuration.
        
        Args:
            config: Scraper-specific configuration from YAML
            cookies_file: Optional path to cookies JSON file
        """
        self.config = config
        self.cookies_file = cookies_file
        self._setup()
    
    @abstractmethod
    def _setup(self) -> None:
        """Setup scraper (sessions, load cookies, etc.)."""
        pass
    
    @abstractmethod
    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search for jobs with given query.
        
        Args:
            query: Query parameters (keywords, location, limit, etc.)
                   Structure varies by scraper
        
        Returns:
            List of JobPosting objects
        """
        pass
    
    @abstractmethod
    def validate_config(self) -> bool:
        """Validate scraper-specific configuration."""
        pass
    
    def get_rate_limit_delay(self) -> float:
        """Get delay between requests (default: 1.5s)."""
        return self.config.get('rate_limit_delay', 1.5)
```

### Example: CV.ee Scraper

```python
# scrapers/cvee.py
from typing import List, Optional, Any
from pathlib import Path
import requests
from datetime import datetime
from pydantic import HttpUrl

from job_monitor.schemas import JobPosting
from job_monitor.scrapers.base import BaseScraper


class CVeeScraper(BaseScraper):
    """CV.ee job scraper using REST API."""
    
    SCRAPER_ID = "cvee"
    DISPLAY_NAME = "CV.ee"
    REQUIRES_COOKIES = False
    REQUIRES_AUTH = False
    BASE_URLS = ["https://cv.ee"]
    
    def _setup(self) -> None:
        """Setup HTTP session."""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Accept': 'application/json',
        })
        
        # Cache locations on first search
        self._locations_cache: Optional[dict] = None
    
    def validate_config(self) -> bool:
        """Validate CV.ee configuration."""
        # CV.ee has no required config (uses public API)
        return True
    
    def _get_locations(self) -> dict:
        """Fetch and cache location mappings."""
        if self._locations_cache is not None:
            return self._locations_cache
        
        response = self.session.get(
            "https://cv.ee/api/v1/locations-service/list",
            timeout=30
        )
        response.raise_for_status()
        
        data = response.json()
        
        # Build townId -> name mapping
        town_map = {town['id']: town['name'] for town in data['towns']}
        
        self._locations_cache = {
            'towns': town_map,
            'counties': data.get('counties', {}),
            'countries': data.get('countries', {})
        }
        
        return self._locations_cache
    
    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search CV.ee for jobs.
        
        Query parameters:
            keywords: str - Search terms
            location: str - City name (e.g., "Tallinn")
            limit: int - Results per page (default: 20)
            categories: List[str] - Category filters (e.g., ["INFORMATION_TECHNOLOGY"])
            salary_from: int - Minimum salary
            sorting: str - "LATEST", "RELEVANCE", "OLDEST"
        """
        # Load locations for mapping
        locations = self._get_locations()
        
        # Build API request
        params = {
            'sorting': query.get('sorting', 'LATEST'),
            'limit': query.get('limit', 20),
            'offset': query.get('offset', 0),
        }
        
        if 'keywords' in query:
            params['keywords'] = query['keywords']
        
        if 'location' in query:
            params['location'] = query['location']
        
        if 'categories' in query:
            # Handle array parameters
            for cat in query['categories']:
                params['categories[]'] = cat
        
        if 'salary_from' in query:
            params['salaryFrom'] = query['salary_from']
        
        # Make API request
        response = self.session.get(
            "https://cv.ee/api/v1/vacancy-search-service/search",
            params=params,
            timeout=30
        )
        response.raise_for_status()
        
        data = response.json()
        
        # Parse results
        jobs = []
        for vacancy in data.get('vacancies', []):
            job = self._parse_vacancy(vacancy, locations)
            if job:
                jobs.append(job)
        
        return jobs
    
    def _parse_vacancy(self, vacancy: dict, locations: dict) -> Optional[JobPosting]:
        """Convert CV.ee vacancy to JobPosting."""
        try:
            # Resolve location
            town_id = vacancy.get('townId')
            location = locations['towns'].get(town_id, 'Unknown')
            
            # Build job URL
            job_id = vacancy['id']
            url = f"https://cv.ee/en/vacancy/{job_id}"
            
            # Extract salary
            salary = None
            if vacancy.get('salaryFrom') and vacancy.get('salaryTo'):
                salary = f"‚Ç¨{int(vacancy['salaryFrom'])}-{int(vacancy['salaryTo'])}"
            elif vacancy.get('salaryFrom'):
                salary = f"‚Ç¨{int(vacancy['salaryFrom'])}+"
            
            return JobPosting(
                url=HttpUrl(url),
                title=vacancy['positionTitle'],
                company=vacancy.get('employerName', 'Unknown'),
                location=location,
                posted_date=vacancy.get('publishDate'),
                discovered_date=datetime.now(),
                description=None,  # CV.ee doesn't include full description in search
                salary=salary,
                remote=vacancy.get('remoteWork', False),
                source=self.DISPLAY_NAME
            )
        except Exception as e:
            print(f"‚ö†Ô∏è  Error parsing CV.ee vacancy: {e}")
            return None
```

### Example: Duunitori Scraper

```python
# scrapers/duunitori.py
from typing import List, Optional, Any
from pathlib import Path
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
from pydantic import HttpUrl

from job_monitor.schemas import JobPosting
from job_monitor.scrapers.base import BaseScraper


class DuunitoriScraper(BaseScraper):
    """Duunitori job scraper using HTML parsing with cookies."""
    
    SCRAPER_ID = "duunitori"
    DISPLAY_NAME = "Duunitori"
    REQUIRES_COOKIES = True  # Needs authentication
    REQUIRES_AUTH = False
    BASE_URLS = ["https://duunitori.fi"]
    
    def _setup(self) -> None:
        """Setup HTTP session and load cookies."""
        self.session = requests.Session()
        
        if self.REQUIRES_COOKIES and not self.cookies_file:
            raise ValueError(f"{self.DISPLAY_NAME} requires cookies file")
        
        if self.cookies_file:
            self._load_cookies()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
    
    def _load_cookies(self) -> None:
        """Load cookies from JSON file."""
        with open(self.cookies_file, 'r') as f:
            cookies_data = json.load(f)
        
        if isinstance(cookies_data, list):
            for cookie in cookies_data:
                self.session.cookies.set(
                    cookie['name'],
                    cookie['value'],
                    domain=cookie.get('domain', '')
                )
        else:
            for name, value in cookies_data.items():
                self.session.cookies.set(name, value)
    
    def validate_config(self) -> bool:
        """Validate Duunitori configuration."""
        if self.REQUIRES_COOKIES and not self.cookies_file:
            return False
        if self.cookies_file and not self.cookies_file.exists():
            return False
        return True
    
    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search Duunitori for jobs.
        
        Query parameters:
            keywords: str - Search terms
            location: str - Location filter
            limit: int - Max results
            full_details: bool - Fetch full job descriptions
        """
        # Existing Duunitori search logic from scraper.py
        # ... (move existing code here)
        pass
```

### Scraper Registry

```python
# scrapers/__init__.py
from typing import Dict, Type, Optional
from pathlib import Path

from job_monitor.scrapers.base import BaseScraper
from job_monitor.scrapers.cvee import CVeeScraper
from job_monitor.scrapers.duunitori import DuunitoriScraper
# from job_monitor.scrapers.linkedin import LinkedInScraper


class ScraperRegistry:
    """Central registry for all available scrapers."""
    
    _scrapers: Dict[str, Type[BaseScraper]] = {}
    
    @classmethod
    def register(cls, scraper_class: Type[BaseScraper]) -> None:
        """Register a scraper class."""
        cls._scrapers[scraper_class.SCRAPER_ID] = scraper_class
    
    @classmethod
    def get_scraper(cls, scraper_id: str, config: dict, cookies_file: Optional[Path] = None) -> BaseScraper:
        """
        Get scraper instance by ID.
        
        Args:
            scraper_id: Scraper identifier (e.g., "cvee", "duunitori")
            config: Scraper-specific configuration
            cookies_file: Optional cookies file path
        
        Returns:
            Initialized scraper instance
        
        Raises:
            ValueError: If scraper ID not found
        """
        if scraper_id not in cls._scrapers:
            raise ValueError(f"Unknown scraper: {scraper_id}")
        
        scraper_class = cls._scrapers[scraper_id]
        return scraper_class(config=config, cookies_file=cookies_file)
    
    @classmethod
    def list_scrapers(cls) -> Dict[str, Type[BaseScraper]]:
        """Get all registered scrapers."""
        return cls._scrapers.copy()


# Auto-register scrapers
ScraperRegistry.register(CVeeScraper)
ScraperRegistry.register(DuunitoriScraper)
# ScraperRegistry.register(LinkedInScraper)
```

### Updated CLI Integration

```python
# cli.py (updated _scrape_source function)
from job_monitor.scrapers import ScraperRegistry

def _scrape_source(
    source_name: str,
    source_config: dict,
    queries: list[dict],
    cookies_file: Optional[Path] = None,
    state_manager: Optional[StateManager] = None
) -> list[JobPosting]:
    """
    Scrape jobs from a source using appropriate scraper.
    
    Args:
        source_name: Source identifier (e.g., "cvee", "duunitori")
        source_config: Source-specific configuration
        queries: List of query dictionaries
        cookies_file: Optional cookies file
        state_manager: Optional state manager
    
    Returns:
        List of JobPosting objects
    """
    try:
        # Get scraper from registry
        scraper = ScraperRegistry.get_scraper(
            scraper_id=source_name,
            config=source_config,
            cookies_file=cookies_file
        )
        
        # Validate configuration
        if not scraper.validate_config():
            print(f"‚ùå Invalid configuration for {scraper.DISPLAY_NAME}")
            return []
        
        # Execute queries
        all_jobs = []
        for query in queries:
            print(f"\nüîç Searching {scraper.DISPLAY_NAME}: {query.get('keywords', 'all jobs')}")
            jobs = scraper.search(query)
            all_jobs.extend(jobs)
        
        # Annotate source
        for job in all_jobs:
            job.source = scraper.DISPLAY_NAME
        
        print(f"‚úÖ Found {len(all_jobs)} jobs from {scraper.DISPLAY_NAME}")
        return all_jobs
        
    except ValueError as e:
        print(f"‚ùå {e}")
        return []
    except Exception as e:
        print(f"‚ùå Error scraping {source_name}: {e}")
        import traceback
        traceback.print_exc()
        return []
```

### Configuration Format

```yaml
# config.yaml
sources:
  - name: cvee
    enabled: true
    scraper: cvee  # Maps to ScraperRegistry
    config:
      rate_limit_delay: 1.5
    queries:
      - keywords: "python developer"
        location: "Tallinn"
        categories: ["INFORMATION_TECHNOLOGY"]
        salary_from: 3000
        sorting: "LATEST"
        limit: 30
      
      - keywords: "software architect"
        location: "Tartu"
        categories: ["INFORMATION_TECHNOLOGY"]
        salary_from: 4000
  
  - name: duunitori
    enabled: true
    scraper: duunitori
    cookies_file: "./cookies/duunitori.json"
    config:
      rate_limit_delay: 2.0
      full_details: true
    queries:
      - keywords: "python kehitt√§j√§"
        location: "Helsinki"
        limit: 20
      
      - keywords: "ohjelmistoarkkitehti"
        location: "Tampere"
        limit: 15
  
  - name: linkedin
    enabled: false
    scraper: linkedin
    cookies_file: "./cookies/linkedin.json"
    queries:
      - keywords: "software engineer"
        location: "Finland"
```

## Benefits of This Approach

### 1. **Separation of Concerns**
- Each scraper is self-contained
- Duunitori changes don't affect CV.ee
- Can use different libraries per scraper

### 2. **Easy to Extend**
```python
# Add new scraper:
# 1. Create scrapers/newsite.py
# 2. Inherit from BaseScraper
# 3. Implement search() and _setup()
# 4. Register in scrapers/__init__.py
# 5. Add to config.yaml
```

### 3. **Easy to Test**
```python
# Test scraper in isolation
def test_cvee_scraper():
    scraper = CVeeScraper(config={}, cookies_file=None)
    jobs = scraper.search({
        'keywords': 'python',
        'location': 'Tallinn',
        'limit': 5
    })
    assert len(jobs) > 0
    assert all(isinstance(j, JobPosting) for j in jobs)
```

### 4. **Configuration-Driven**
- Enable/disable scrapers without code changes
- Different queries per scraper
- Scraper-specific settings isolated

### 5. **Backward Compatible**
- Keep old `scraper.py` for transition
- Gradually migrate sources
- Both approaches work during migration

### 6. **Shared Infrastructure**
Common utilities in `scrapers/utils.py`:
```python
# Rate limiting decorator
@rate_limit(delay=1.5)
def make_request(url):
    ...

# Retry logic
@retry(max_attempts=3, backoff=2)
def fetch_with_retry(url):
    ...

# Caching
@cached(ttl=3600)
def get_static_data():
    ...
```

## Migration Path

### Phase 1: Setup Structure ‚úÖ
1. Create `scrapers/` directory
2. Create `base.py` with `BaseScraper`
3. Create registry in `__init__.py`

### Phase 2: Migrate CV.ee ‚úÖ
1. Create `scrapers/cvee.py`
2. Implement using REST API research
3. Register in registry
4. Test independently

### Phase 3: Migrate Duunitori
1. Move existing code to `scrapers/duunitori.py`
2. Adapt to `BaseScraper` interface
3. Keep functionality identical
4. Test against existing config

### Phase 4: Update CLI
1. Update `_scrape_source()` to use registry
2. Keep backward compatibility
3. Update config format (optional)

### Phase 5: Add More Scrapers
1. LinkedIn (if needed)
2. cv.lv / cvonline.lt (reuse CV.ee logic)
3. Others as required

## Testing Strategy

```python
# tests/test_scrapers/test_cvee.py
import pytest
from job_monitor.scrapers import ScraperRegistry

def test_cvee_registration():
    """Test CV.ee scraper is registered."""
    scrapers = ScraperRegistry.list_scrapers()
    assert 'cvee' in scrapers

def test_cvee_search_basic():
    """Test basic CV.ee search."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({
        'keywords': 'python',
        'limit': 5
    })
    assert len(jobs) <= 5
    assert all(j.source == 'CV.ee' for j in jobs)

def test_cvee_search_with_filters():
    """Test CV.ee search with filters."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({
        'keywords': 'developer',
        'location': 'Tallinn',
        'categories': ['INFORMATION_TECHNOLOGY'],
        'salary_from': 3000
    })
    assert all(j.location == 'Tallinn' for j in jobs)

@pytest.mark.integration
def test_cvee_live_api():
    """Integration test against live CV.ee API."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({'keywords': 'python', 'limit': 1})
    assert len(jobs) > 0
```

## Alternatives Considered

### Alternative 1: Keep Monolithic Class
**Pros:**
- No refactoring needed
- Familiar structure

**Cons:**
- ‚ùå Single file grows to 1000+ lines
- ‚ùå Tight coupling between scrapers
- ‚ùå Hard to test individual scrapers
- ‚ùå Dependencies mixed (BeautifulSoup + others)

### Alternative 2: Separate Files, Manual Routing
**Pros:**
- Better organization than monolith

**Cons:**
- ‚ùå Still hardcoded routing in CLI
- ‚ùå No common interface
- ‚ùå Each file implements differently

### Alternative 3: Plugin System with Entry Points
**Pros:**
- True plugin architecture
- Scrapers can be separate packages

**Cons:**
- ‚ùå Overkill for current needs
- ‚ùå More complex setup
- ‚ùå Harder for single developer

**Recommendation:** Plugin architecture with registry (Proposed approach) - good balance of flexibility and simplicity.

## Conclusion

The **plugin architecture with scraper registry** provides:
- ‚úÖ Clean separation of concerns
- ‚úÖ Easy to add new scrapers
- ‚úÖ Easy to maintain existing scrapers
- ‚úÖ Configuration-driven behavior
- ‚úÖ Testable in isolation
- ‚úÖ Backward compatible migration path

This prevents "the mess" by ensuring each scraper:
- Lives in its own file
- Has clear responsibilities
- Shares common interface
- Can evolve independently
- Is easy to enable/disable

## Next Steps

1. Create `scrapers/` directory structure
2. Implement `base.py` with `BaseScraper`
3. Implement `cvee.py` scraper
4. Implement scraper registry
5. Test CV.ee scraper independently
6. Update CLI to use registry
7. Migrate Duunitori scraper
8. Document scraper creation guide

## Related Issues

- [Issue #24: Add CV.ee job portal support](https://github.com/mitselek/cv_system/issues/24)
