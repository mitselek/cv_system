# Job Scraper Architecture

**Date:** December 2, 2025  
**Status:** âœ… IMPLEMENTED (Milestone #2 Complete)  
**Version:** 1.0  
**Problem:** Avoiding "the mess" when maintaining multiple scrapers with different approaches

## Implementation Summary

The plugin architecture with scraper registry has been successfully implemented! The system now supports:

- âœ… **Duunitori** - HTML scraping with cookies (326 lines, 20 tests)
- âœ… **CV.ee** - REST API integration (362 lines, 19 tests)  
- âœ… **Registry system** - Dynamic scraper loading (187 lines, 23 tests)
- âœ… **CLI integration** - Unified command interface (461 lines, 19 tests)
- âœ… **Integration tests** - End-to-end workflows (14 tests)

**Total:** 209 tests passing, 100% success rate

### Quick Start

```python
from job_monitor.scrapers import ScraperRegistry

# List available scrapers
scrapers = ScraperRegistry.list_scrapers()
# {'duunitori': <class 'DuunitoriScraper'>, 'cvee': <class 'CVeeScraper'>}

# Get scraper instance
scraper = ScraperRegistry.get_scraper('cvee', config={})

# Search for jobs
jobs = scraper.search({
    'keywords': 'python developer',
    'location': 'Tallinn',
    'limit': 20
})
```

### CLI Usage

```bash
# Scan with multiple scrapers
job-monitor scan --config config.yaml

# List available scrapers
python -c "from job_monitor.scrapers import ScraperRegistry; print(ScraperRegistry.list_scrapers())"
```

---

## Original Design Proposal

**Date:** December 2, 2025  
**Status:** Design Proposal

## Current State Analysis

### Existing Structure

```text
job-monitoring/src/job_monitor/
â”œâ”€â”€ scrapers/                    # Scraper plugins (IMPLEMENTED)
â”‚   â”œâ”€â”€ __init__.py             # Scraper registry
â”‚   â”œâ”€â”€ base.py                 # Abstract base class
â”‚   â”œâ”€â”€ duunitori.py            # Duunitori scraper
â”‚   â””â”€â”€ cvee.py                 # CV.ee scraper
â”œâ”€â”€ cli.py                       # Uses registry (UPDATED)
â”œâ”€â”€ config.py                    # No changes needed
â””â”€â”€ schemas.py                   # ScraperConfig added
```

### Current Problems

1. **Single Monolithic Class**

   - `JobScraper` contains methods for all portals
   - `search_duunitori()`, `search_linkedin()`, etc. in one file
   - Each portal needs different dependencies (BeautifulSoup vs JSON API)

2. **Hardcoded Routing**

   - `cli.py` has string matching: `if "duunitori" in name`
   - Adding new portal = editing multiple files
   - No clear extension point

3. **Mixed Concerns**

   - Cookie handling (Duunitori needs it)
   - REST API calls (CV.ee doesn't need cookies)
   - HTML parsing vs JSON parsing
   - Rate limiting strategies differ per portal

4. **Future Pain Points**
   - **Duunitori:** HTML scraping with cookies, needs BeautifulSoup
   - **CV.ee:** REST API, no auth, JSON responses
   - **LinkedIn:** Complex auth, might need Selenium
   - **cvonline.lt/cv.lv:** Same API as CV.ee but different URLs
   - Each evolves independently on their own schedule

## Design Principles

### 1. Plugin Architecture

Each scraper is a **self-contained module** that can be:

- Developed independently
- Tested in isolation
- Enabled/disabled without code changes
- Replaced without affecting others

### 2. Clear Abstraction

Define **what** each scraper must do (interface), not **how** they do it:

- Search for jobs with query parameters
- Return standardized `JobPosting` objects
- Handle their own rate limiting
- Manage their own authentication

### 3. Configuration-Driven

Scraper selection based on **configuration**, not code:

- Add new scraper = drop in new file + config entry
- No editing existing code
- Runtime discovery of available scrapers

### 4. Shared Infrastructure

Common utilities available to all scrapers:

- HTTP session management
- Rate limiting decorators
- Caching
- Error handling patterns

## Proposed Architecture

### Directory Structure

```text
job-monitoring/src/job_monitor/
â”œâ”€â”€ scrapers/                    # NEW: Scraper plugins
â”‚   â”œâ”€â”€ __init__.py             # Scraper registry
â”‚   â”œâ”€â”€ base.py                 # Abstract base class
â”‚   â”œâ”€â”€ duunitori.py            # Duunitori scraper
â”‚   â”œâ”€â”€ cvee.py                 # CV.ee scraper
â”‚   â”œâ”€â”€ cvonline.py             # cv.lv + cvonline.lt (shared API)
â”‚   â””â”€â”€ linkedin.py             # LinkedIn scraper
â”œâ”€â”€ cli.py                       # Update to use registry
â”œâ”€â”€ config.py                    # No changes needed
â””â”€â”€ schemas.py                   # Add ScraperConfig model
```

### Base Scraper Interface

```python
# scrapers/base.py
from abc import ABC, abstractmethod
from typing import List, Optional, Any
from pathlib import Path
from job_monitor.schemas import JobPosting

class BaseScraper(ABC):
    """Abstract base class for all job scrapers."""

    # Class-level metadata
    SCRAPER_ID: str          # e.g., "cvee", "duunitori"
    DISPLAY_NAME: str        # e.g., "CV.ee", "Duunitori"
    REQUIRES_COOKIES: bool = False
    REQUIRES_AUTH: bool = False
    BASE_URLS: List[str]     # e.g., ["https://cv.ee"]

    def __init__(self, config: dict[str, Any], cookies_file: Optional[Path] = None):
        """
        Initialize scraper with configuration.

        Args:
            config: Scraper-specific configuration from YAML
            cookies_file: Optional path to cookies JSON file
        """
        self.config = config
        self.cookies_file = cookies_file
        self._setup()

    @abstractmethod
    def _setup(self) -> None:
        """Setup scraper (sessions, load cookies, etc.)."""
        pass

    @abstractmethod
    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search for jobs with given query.

        Args:
            query: Query parameters (keywords, location, limit, etc.)
                   Structure varies by scraper

        Returns:
            List of JobPosting objects
        """
        pass

    @abstractmethod
    def validate_config(self) -> bool:
        """Validate scraper-specific configuration."""
        pass

    def get_rate_limit_delay(self) -> float:
        """Get delay between requests (default: 1.5s)."""
        return self.config.get('rate_limit_delay', 1.5)
```

### Example: CV.ee Scraper

```python
# scrapers/cvee.py
from typing import List, Optional, Any
from pathlib import Path
import requests
from datetime import datetime
from pydantic import HttpUrl

from job_monitor.schemas import JobPosting
from job_monitor.scrapers.base import BaseScraper


class CVeeScraper(BaseScraper):
    """CV.ee job scraper using REST API."""

    SCRAPER_ID = "cvee"
    DISPLAY_NAME = "CV.ee"
    REQUIRES_COOKIES = False
    REQUIRES_AUTH = False
    BASE_URLS = ["https://cv.ee"]

    def _setup(self) -> None:
        """Setup HTTP session."""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Accept': 'application/json',
        })

        # Cache locations on first search
        self._locations_cache: Optional[dict] = None

    def validate_config(self) -> bool:
        """Validate CV.ee configuration."""
        # CV.ee has no required config (uses public API)
        return True

    def _get_locations(self) -> dict:
        """Fetch and cache location mappings."""
        if self._locations_cache is not None:
            return self._locations_cache

        response = self.session.get(
            "https://cv.ee/api/v1/locations-service/list",
            timeout=30
        )
        response.raise_for_status()

        data = response.json()

        # Build townId -> name mapping
        town_map = {town['id']: town['name'] for town in data['towns']}

        self._locations_cache = {
            'towns': town_map,
            'counties': data.get('counties', {}),
            'countries': data.get('countries', {})
        }

        return self._locations_cache

    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search CV.ee for jobs.

        Query parameters:
            keywords: str - Search terms
            location: str - City name (e.g., "Tallinn")
            limit: int - Results per page (default: 20)
            categories: List[str] - Category filters (e.g., ["INFORMATION_TECHNOLOGY"])
            salary_from: int - Minimum salary
            sorting: str - "LATEST", "RELEVANCE", "OLDEST"
        """
        # Load locations for mapping
        locations = self._get_locations()

        # Build API request
        params = {
            'sorting': query.get('sorting', 'LATEST'),
            'limit': query.get('limit', 20),
            'offset': query.get('offset', 0),
        }

        if 'keywords' in query:
            params['keywords'] = query['keywords']

        if 'location' in query:
            params['location'] = query['location']

        if 'categories' in query:
            # Handle array parameters
            for cat in query['categories']:
                params['categories[]'] = cat

        if 'salary_from' in query:
            params['salaryFrom'] = query['salary_from']

        # Make API request
        response = self.session.get(
            "https://cv.ee/api/v1/vacancy-search-service/search",
            params=params,
            timeout=30
        )
        response.raise_for_status()

        data = response.json()

        # Parse results
        jobs = []
        for vacancy in data.get('vacancies', []):
            job = self._parse_vacancy(vacancy, locations)
            if job:
                jobs.append(job)

        return jobs

    def _parse_vacancy(self, vacancy: dict, locations: dict) -> Optional[JobPosting]:
        """Convert CV.ee vacancy to JobPosting."""
        try:
            # Resolve location
            town_id = vacancy.get('townId')
            location = locations['towns'].get(town_id, 'Unknown')

            # Build job URL
            job_id = vacancy['id']
            url = f"https://cv.ee/en/vacancy/{job_id}"

            # Extract salary
            salary = None
            if vacancy.get('salaryFrom') and vacancy.get('salaryTo'):
                salary = f"â‚¬{int(vacancy['salaryFrom'])}-{int(vacancy['salaryTo'])}"
            elif vacancy.get('salaryFrom'):
                salary = f"â‚¬{int(vacancy['salaryFrom'])}+"

            return JobPosting(
                url=HttpUrl(url),
                title=vacancy['positionTitle'],
                company=vacancy.get('employerName', 'Unknown'),
                location=location,
                posted_date=vacancy.get('publishDate'),
                discovered_date=datetime.now(),
                description=None,  # CV.ee doesn't include full description in search
                salary=salary,
                remote=vacancy.get('remoteWork', False),
                source=self.DISPLAY_NAME
            )
        except Exception as e:
            print(f"âš ï¸  Error parsing CV.ee vacancy: {e}")
            return None
```

### Example: Duunitori Scraper

```python
# scrapers/duunitori.py
from typing import List, Optional, Any
from pathlib import Path
import requests
import json
from datetime import datetime
from bs4 import BeautifulSoup
from pydantic import HttpUrl

from job_monitor.schemas import JobPosting
from job_monitor.scrapers.base import BaseScraper


class DuunitoriScraper(BaseScraper):
    """Duunitori job scraper using HTML parsing with cookies."""

    SCRAPER_ID = "duunitori"
    DISPLAY_NAME = "Duunitori"
    REQUIRES_COOKIES = True  # Needs authentication
    REQUIRES_AUTH = False
    BASE_URLS = ["https://duunitori.fi"]

    def _setup(self) -> None:
        """Setup HTTP session and load cookies."""
        self.session = requests.Session()

        if self.REQUIRES_COOKIES and not self.cookies_file:
            raise ValueError(f"{self.DISPLAY_NAME} requires cookies file")

        if self.cookies_file:
            self._load_cookies()

        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })

    def _load_cookies(self) -> None:
        """Load cookies from JSON file."""
        with open(self.cookies_file, 'r') as f:
            cookies_data = json.load(f)

        if isinstance(cookies_data, list):
            for cookie in cookies_data:
                self.session.cookies.set(
                    cookie['name'],
                    cookie['value'],
                    domain=cookie.get('domain', '')
                )
        else:
            for name, value in cookies_data.items():
                self.session.cookies.set(name, value)

    def validate_config(self) -> bool:
        """Validate Duunitori configuration."""
        if self.REQUIRES_COOKIES and not self.cookies_file:
            return False
        if self.cookies_file and not self.cookies_file.exists():
            return False
        return True

    def search(self, query: dict[str, Any]) -> List[JobPosting]:
        """
        Search Duunitori for jobs.

        Query parameters:
            keywords: str - Search terms
            location: str - Location filter
            limit: int - Max results
            full_details: bool - Fetch full job descriptions
        """
        # Existing Duunitori search logic from scraper.py
        # ... (move existing code here)
        pass
```

### Scraper Registry

```python
# scrapers/__init__.py
from typing import Dict, Type, Optional
from pathlib import Path

from job_monitor.scrapers.base import BaseScraper
from job_monitor.scrapers.cvee import CVeeScraper
from job_monitor.scrapers.duunitori import DuunitoriScraper
# from job_monitor.scrapers.linkedin import LinkedInScraper


class ScraperRegistry:
    """Central registry for all available scrapers."""

    _scrapers: Dict[str, Type[BaseScraper]] = {}

    @classmethod
    def register(cls, scraper_class: Type[BaseScraper]) -> None:
        """Register a scraper class."""
        cls._scrapers[scraper_class.SCRAPER_ID] = scraper_class

    @classmethod
    def get_scraper(cls, scraper_id: str, config: dict, cookies_file: Optional[Path] = None) -> BaseScraper:
        """
        Get scraper instance by ID.

        Args:
            scraper_id: Scraper identifier (e.g., "cvee", "duunitori")
            config: Scraper-specific configuration
            cookies_file: Optional cookies file path

        Returns:
            Initialized scraper instance

        Raises:
            ValueError: If scraper ID not found
        """
        if scraper_id not in cls._scrapers:
            raise ValueError(f"Unknown scraper: {scraper_id}")

        scraper_class = cls._scrapers[scraper_id]
        return scraper_class(config=config, cookies_file=cookies_file)

    @classmethod
    def list_scrapers(cls) -> Dict[str, Type[BaseScraper]]:
        """Get all registered scrapers."""
        return cls._scrapers.copy()


# Auto-register scrapers
ScraperRegistry.register(CVeeScraper)
ScraperRegistry.register(DuunitoriScraper)
# ScraperRegistry.register(LinkedInScraper)
```

### Updated CLI Integration

```python
# cli.py (updated _scrape_source function)
from job_monitor.scrapers import ScraperRegistry

def _scrape_source(
    source_name: str,
    source_config: dict,
    queries: list[dict],
    cookies_file: Optional[Path] = None,
    state_manager: Optional[StateManager] = None
) -> list[JobPosting]:
    """
    Scrape jobs from a source using appropriate scraper.

    Args:
        source_name: Source identifier (e.g., "cvee", "duunitori")
        source_config: Source-specific configuration
        queries: List of query dictionaries
        cookies_file: Optional cookies file
        state_manager: Optional state manager

    Returns:
        List of JobPosting objects
    """
    try:
        # Get scraper from registry
        scraper = ScraperRegistry.get_scraper(
            scraper_id=source_name,
            config=source_config,
            cookies_file=cookies_file
        )

        # Validate configuration
        if not scraper.validate_config():
            print(f"âŒ Invalid configuration for {scraper.DISPLAY_NAME}")
            return []

        # Execute queries
        all_jobs = []
        for query in queries:
            print(f"\nğŸ” Searching {scraper.DISPLAY_NAME}: {query.get('keywords', 'all jobs')}")
            jobs = scraper.search(query)
            all_jobs.extend(jobs)

        # Annotate source
        for job in all_jobs:
            job.source = scraper.DISPLAY_NAME

        print(f"âœ… Found {len(all_jobs)} jobs from {scraper.DISPLAY_NAME}")
        return all_jobs

    except ValueError as e:
        print(f"âŒ {e}")
        return []
    except Exception as e:
        print(f"âŒ Error scraping {source_name}: {e}")
        import traceback
        traceback.print_exc()
        return []
```

### Configuration Format

```yaml
# config.yaml
sources:
  - name: cvee
    enabled: true
    scraper: cvee # Maps to ScraperRegistry
    config:
      rate_limit_delay: 1.5
    queries:
      - keywords: "python developer"
        location: "Tallinn"
        categories: ["INFORMATION_TECHNOLOGY"]
        salary_from: 3000
        sorting: "LATEST"
        limit: 30

      - keywords: "software architect"
        location: "Tartu"
        categories: ["INFORMATION_TECHNOLOGY"]
        salary_from: 4000

  - name: duunitori
    enabled: true
    scraper: duunitori
    cookies_file: "./cookies/duunitori.json"
    config:
      rate_limit_delay: 2.0
      full_details: true
    queries:
      - keywords: "python kehittÃ¤jÃ¤"
        location: "Helsinki"
        limit: 20

      - keywords: "ohjelmistoarkkitehti"
        location: "Tampere"
        limit: 15

  - name: linkedin
    enabled: false
    scraper: linkedin
    cookies_file: "./cookies/linkedin.json"
    queries:
      - keywords: "software engineer"
        location: "Finland"
```

## Benefits of This Approach

### 1. **Separation of Concerns**

- Each scraper is self-contained
- Duunitori changes don't affect CV.ee
- Can use different libraries per scraper

### 2. **Easy to Extend**

```python
# Add new scraper:
# 1. Create scrapers/newsite.py
# 2. Inherit from BaseScraper
# 3. Implement search() and _setup()
# 4. Register in scrapers/__init__.py
# 5. Add to config.yaml
```

### 3. **Easy to Test**

```python
# Test scraper in isolation
def test_cvee_scraper():
    scraper = CVeeScraper(config={}, cookies_file=None)
    jobs = scraper.search({
        'keywords': 'python',
        'location': 'Tallinn',
        'limit': 5
    })
    assert len(jobs) > 0
    assert all(isinstance(j, JobPosting) for j in jobs)
```

### 4. **Configuration-Driven**

- Enable/disable scrapers without code changes
- Different queries per scraper
- Scraper-specific settings isolated

### 5. **Backward Compatible**

- Keep old `scraper.py` for transition
- Gradually migrate sources
- Both approaches work during migration

### 6. **Shared Infrastructure**

Common utilities in `scrapers/utils.py`:

```python
# Rate limiting decorator
@rate_limit(delay=1.5)
def make_request(url):
    ...

# Retry logic
@retry(max_attempts=3, backoff=2)
def fetch_with_retry(url):
    ...

# Caching
@cached(ttl=3600)
def get_static_data():
    ...
```

## Migration Path

**âœ… COMPLETED - All phases implemented:**

### Phase 1: Setup Structure âœ…

1. âœ… Created `scrapers/` directory
2. âœ… Created `base.py` with `BaseScraper`
3. âœ… Created registry in `__init__.py`

### Phase 2: Migrate Duunitori âœ…

1. âœ… Moved existing code to `scrapers/duunitori.py`
2. âœ… Adapted to `BaseScraper` interface
3. âœ… Enhanced with robust contact extraction (regex fallbacks)
4. âœ… Tested against existing config

### Phase 3: Migrate CV.ee âœ…

1. âœ… Created `scrapers/cvee.py`
2. âœ… Implemented using REST API research
3. âœ… Registered in registry
4. âœ… Tested independently

### Phase 4: Update CLI âœ…

1. âœ… Updated `_scrape_source()` to use registry
2. âœ… Updated config format
3. âœ… Removed old monolithic scraper.py

### Phase 5: Comprehensive Testing âœ…

1. âœ… Deleted obsolete test files (3 files, 18 tests)
2. âœ… Created new comprehensive tests (12 tests)
3. âœ… Enhanced contact extraction with regex fallbacks
4. âœ… All 206 tests passing

### Phase 5: Add More Scrapers

1. LinkedIn (if needed)
2. cv.lv / cvonline.lt (reuse CV.ee logic)
3. Others as required

## Testing Strategy

```python
# tests/test_scrapers/test_cvee.py
import pytest
from job_monitor.scrapers import ScraperRegistry

def test_cvee_registration():
    """Test CV.ee scraper is registered."""
    scrapers = ScraperRegistry.list_scrapers()
    assert 'cvee' in scrapers

def test_cvee_search_basic():
    """Test basic CV.ee search."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({
        'keywords': 'python',
        'limit': 5
    })
    assert len(jobs) <= 5
    assert all(j.source == 'CV.ee' for j in jobs)

def test_cvee_search_with_filters():
    """Test CV.ee search with filters."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({
        'keywords': 'developer',
        'location': 'Tallinn',
        'categories': ['INFORMATION_TECHNOLOGY'],
        'salary_from': 3000
    })
    assert all(j.location == 'Tallinn' for j in jobs)

@pytest.mark.integration
def test_cvee_live_api():
    """Integration test against live CV.ee API."""
    scraper = ScraperRegistry.get_scraper('cvee', config={})
    jobs = scraper.search({'keywords': 'python', 'limit': 1})
    assert len(jobs) > 0
```

## Alternatives Considered

### Alternative 1: Keep Monolithic Class

**Pros:**

- No refactoring needed
- Familiar structure

**Cons:**

- âŒ Single file grows to 1000+ lines
- âŒ Tight coupling between scrapers
- âŒ Hard to test individual scrapers
- âŒ Dependencies mixed (BeautifulSoup + others)

### Alternative 2: Separate Files, Manual Routing

**Pros:**

- Better organization than monolith

**Cons:**

- âŒ Still hardcoded routing in CLI
- âŒ No common interface
- âŒ Each file implements differently

### Alternative 3: Plugin System with Entry Points

**Pros:**

- True plugin architecture
- Scrapers can be separate packages

**Cons:**

- âŒ Overkill for current needs
- âŒ More complex setup
- âŒ Harder for single developer

**Recommendation:** Plugin architecture with registry (Proposed approach) - good balance of flexibility and simplicity.

## Conclusion

The **plugin architecture with scraper registry** provides:

- âœ… Clean separation of concerns
- âœ… Easy to add new scrapers
- âœ… Easy to maintain existing scrapers
- âœ… Configuration-driven behavior
- âœ… Testable in isolation
- âœ… Backward compatible migration path

This prevents "the mess" by ensuring each scraper:

- Lives in its own file
- Has clear responsibilities
- Shares common interface
- Can evolve independently
- Is easy to enable/disable

## Next Steps

1. Create `scrapers/` directory structure
2. Implement `base.py` with `BaseScraper`
3. Implement `cvee.py` scraper
4. Implement scraper registry
5. Test CV.ee scraper independently
6. Update CLI to use registry
7. Migrate Duunitori scraper
8. Document scraper creation guide

## Related Issues

- [Issue #24: Add CV.ee job portal support](https://github.com/mitselek/cv_system/issues/24)
